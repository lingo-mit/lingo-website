---
layout: default
title: Papers
---

## Papers

### Preprints

- [Bayesian preference elicitation with language models](https://arxiv.org/abs/2403.05534)

  Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex
  Tamkin, Belinda Z. Li.

- [Deductive closure training of language models for coherence, accuracy and updatability.](https://arxiv.org/abs/2401.08574)

  Afra Feyza Aky&uuml;rek, Ekin Aky&uuml;rek, Leshem Choshen, Derry Wijaya,
  Jacob Andreas.

- [Eliciting human preferences with language models.](https://arxiv.org/abs/2310.11589)

  Belinda Z. Li, Alex Tamkin, Noah Goodman and Jacob Andreas.

  <span class="press">Press: <a
href="https://venturebeat.com/ai/how-can-ai-better-understand-humans-simple-by-asking-us-questions/">VentureBeat</a></span>

- [Inspecting and editing knowledge representations in language
  models.](https://arxiv.org/abs/2304.00740)

  Evan Hernandez, Belinda Z. Li and Jacob Andreas.

- [Language models trained on media diets can predict public
  opinion.](https://arxiv.org/abs/2303.16779)

  Eric Chu, Jacob Andreas, Stephen Ansolabehere and Deb Roy.


### 2024

- [In-context language learning: Architectures and algorithms](https://arxiv.org/abs/2401.12973)

  Ekin Aky&uuml;rek, Bailin Wang, Yoon Kim and Jacob Andreas.

  ICML 2024.

- [A multimodal automated interpretability agent](https://arxiv.org/abs/2404.14394)

  Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan
  Hernandez, Jacob Andreas, Antonio Torralba.

  ICML 2024.

  <a class="project" href="https://multimodal-interpretability.csail.mit.edu/maia/">project page</a>

- [Decomposing uncertainty for large language models through input clarification
  ensembling](https://arxiv.org/abs/2311.08718)

  Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang and Yang Zhang.

  ICML 2024.

- [Learning phonotactics from linguistic informants.](https://lingbuzz.net/lingbuzz/007903)

  Canaan Breiss\*, Alexis Ross\*, Amani Maina-Kilaas, Roger P. Levy, Jacob Andreas.

  SCiL 2024.

- [Contextual and combinatorial structure in sperm whale
  vocalizations.](https://www.biorxiv.org/content/10.1101/2023.12.06.570484v1)

  Pratyusha Sharma, Shane Gero, Roger Payne, David F. Gruber, Daniela Rus\*,
  Antonio Torralba\*, Jacob Andreas\*.

  Nature Communications (in press).

- [Regularized conventions: Equilibrium computation as a model of pragmatic
  reasoning.](https://arxiv.org/abs/2311.09712)

  Athul Paul Jacob, Gabriele Farina and Jacob Andreas.

  NAACL 2024, SCiL 2024.

- [Visual grounding helps learn word meanings in low-data regimes.](https://arxiv.org/abs/2310.13257)

  Chengxu Zhuang, Evelina Fedorenko and Jacob Andreas.

  NAACL 2024.

- [Reasoning or reciting? Exploring the capabilities and limitations of language
  models through counterfactual evaluations.](https://arxiv.org/abs/2307.02477)

  Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang,
  Najoung Kim, Jacob Andreas and Yoon Kim.

  NAACL 2024.

- [LaMPP: Language models as probabilistic priors for perception and action.](https://arxiv.org/abs/2302.02801)

  Belinda Z. Li, William Chen, Pratyusha Sharma and Jacob Andreas.

  ICLR 2024 Workshop on Generative AI for Decision-Making.

- [The consensus game: Language model generation via equilibrium
  search.](https://arxiv.org/abs/2310.09139)

  Athul Paul Jacob, Yikang Shen, Gabriele Farina and Jacob Andreas. 
  

  ICLR 2024 **Spotlight**, NeurIPS R0-FoMo Workshop **Best Paper**.

- [Linearity of relation decoding in transformer language
  models.](https://arxiv.org/abs/2308.09124)

  Evan Hernandez\*, Arnab Sen Sharma\*, Tal Haklay, Kevin Meng, Martin Wattenberg,
  Jacob Andreas, Yonatan Belinkov and David Bau. 

  ICLR 2024 **Spotlight**.

  <span class="press">
    Press:
    <a href="https://news.ycombinator.com/item?id=39852118">Hacker News</a>,
    <a href="https://news.mit.edu/2024/large-language-models-use-surprisingly-simple-mechanism-retrieve-stored-knowledge-0325">MIT News</a>.
  </span>

- [Learning with language-guided state abstractions.](https://social-intelligence-human-ai.github.io/docs/1.pdf)

  Andi Peng, Ilia Sucholutsky, Belinda Z. Li, Theodore Sumers, Thomas L.
  Griffiths, Jacob Andreas and Julie Shah.

  ICLR 2024.

  <span class="press">
    Press:
    <a href="https://news.mit.edu/2024/natural-language-boosts-llm-performance-coding-planning-robotics-0501">MIT News</a>
  </span>

- [Learning adaptive planning representations with natural language
  guidance](https://arxiv.org/abs/2312.08566)

  Lionel Wong, Jiayuan Mao, Pratyusha Sharma, Zachary S. Siegel, Jiahai Feng,
  Noa Korneev, Joshua B. Tenenbaum and Jacob Andreas. 

  ICLR 2024.

  <span class="press">
    Press:
    <a href="https://news.mit.edu/2024/natural-language-boosts-llm-performance-coding-planning-robotics-0501">MIT News</a>
  </span>

- [Modeling boundedly rational agents with latent inference
  budgets.](https://arxiv.org/abs/2312.04030)

  Athul Paul Jacob, Abhishek Gupta and Jacob Andreas. 

  ICLR 2024.

  <span class="press">
    Press:
    <a href="https://news.mit.edu/2024/building-better-ai-helper-starts-with-modeling-irrational-behavior-0419">MIT News</a>.
  </span>

- [LILO: Learning interpretable libraries by compressing and documenting code.](https://arxiv.org/abs/2310.19791)

  Gabriel Grand, Lionel Wong, Maddy Bowers, Theo X. Olausson, Muxin Liu, Joshua
  B. Tenenbaum and Jacob Andreas.

  ICLR 2024.

  <span class="press">
    Press:
    <a href="https://news.mit.edu/2024/natural-language-boosts-llm-performance-coding-planning-robotics-0501">MIT News</a>
  </span>

### 2023

- [Alignment via mutual information.](https://aclanthology.org/2023.conll-1.32/)

  Shinjini Ghosh, Yoon Kim, Ramon Fernandez Astudillo, Tahira Naseem and Jacob
  Andreas.

  CoNLL 2023.

  <a class="code" href="https://github.com/jacobandreas/info_align">code</a>

- [Cognitive dissonance: Why do language model outputs disagree with internal
  representations of truthfulness?](https://arxiv.org/abs/2312.03729)

  Kevin Liu, Stephen Casper, Dylan Hadfield-Menell and Jacob Andreas.

  EMNLP 2023.

  <a class="code" href="https://github.com/lingo-mit/lm-truthfulness">code</a>

- [Pushdown layers: Encoding recursive structure in transformer language models.](https://arxiv.org/abs/2310.19089)

  Shikhar Murty, Pratyusha Sharma, Jacob Andreas and Christopher D. Manning.
  
  EMNLP 2023.

  <a class="code" href="https://github.com/MurtyShikhar/Pushdown-Layers">code</a>

- [AutoReply: Detecting nonsense in dialogue introspectively with discriminative replies.](https://arxiv.org/abs/2211.12615)

  Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis.

  EMNLP Findings 2023.

- [Pseudointelligence: A unifying framework for language model evaluation.](https://arxiv.org/abs/2310.12135)

  Shikhar Murty\*, Orr Paradise\*, Pratyusha Sharma\*.

  EMNLP Findings 2023.

- [The clock and the pizza: Two stories in mechanistic explanation of neural networks.](https://arxiv.org/abs/2306.17844)

  Ziming Liu\*, Ziqian Zhong\*, Max Tegmark and Jacob Andreas.
  
  NeurIPS 2023 <b>Oral</b>.

  <span class="press">
    Press:
    <a href="https://www.quantamagazine.org/how-do-machines-grok-data-20240412/">Quanta</a>
  </span>
  <a class="code" href="https://github.com/fjzzq2002/pizza">code</a>

- [A function interpretation benchmark for evaluating interpretability methods.](https://arxiv.org/abs/2309.03886)

  Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang
  Li, Jacob Andreas, David Bau, Antonio Torralba.
  NeurIPS Datasets & Benchmarks 2023.

  <span class="press">
    Press:
    <a href="https://news.mit.edu/2024/ai-agents-help-explain-other-ai-systems-0103">MIT News</a>.
  </span>
  <a class="project" href="https://multimodal-interpretability.csail.mit.edu/FIND-benchmark/">project page</a>

- [Lexical semantic content, not syntactic structure, is the main contributor to
  ANN-brain similarity of fMRI responses in the language network.](https://direct.mit.edu/nol/article/doi/10.1162/nol_a_00116/116784/Lexical-semantic-content-not-syntactic-structure)

  Carina Kauf, Greta Tuckute, Roger Levy, Jacob Andreas and Evelina Fedorenko.
  
  Neurobiology of Language 2023.

  <span class="press">Press: <a
href="https://www.theatlantic.com/technology/archive/2023/05/llm-ai-chatgpt-neuroscience/674216/">The Atlantic</a></span>
  <a class="code" href="https://github.com/carina-kauf/perturbed-neural-nlp">code</a>

- [Compositionality as lexical symmetry](https://arxiv.org/abs/2201.12926). 

  Ekin Aky&uuml;rek and Jacob Andreas.

  ACL 2023 **Lexical Semantics Area Award**.

  <a class="code" href="https://github.com/ekinakyurek/lexsym">code</a>

- [Grokking of hierarchical structure in vanilla transformers.](https://arxiv.org/abs/2305.18741)

  Shikhar Murty, Pratyusha Sharma, Jacob Andreas and Chris Manning.

  ACL 2023.

  <a class="code" href="https://github.com/MurtyShikhar/structural-grokking">code</a>

- [Language modeling with latent situations.](https://arxiv.org/abs/2212.10012) 

  Belinda Z. Li, Max Nye and Jacob Andreas.

  ACL Findings 2023.

- [Guiding pretraining in reinforcement learning with large language models.](https://arxiv.org/abs/2302.06692)

  Yuqing Du\*, Olivia Watkins\*, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter
  Abbeel, Abhishek Gupta and Jacob Andreas.

  ICML 2023.

  <a class="code" href="https://github.com/yuqingd/ellm">code</a>

- [PromptBoosting: Text classification with langauge models in ten forward passes.](https://arxiv.org/abs/2212.0925://arxiv.org/abs/2212.09257) 

  Bairu Hou, Joe O’Connor, Jacob Andreas, Yang Zhang and Shiyu Chang.

  ICML, 2023.

- [What learning algorithm is in-context learning? Investigations with linear models.](https://arxiv.org/abs/2211.15661)

  Ekin Aky&uuml;rek, Dale Schuurmans, Jacob Andreas\*, Tengyu Ma\*, Denny Zhou\*.

  ICLR 2023 <b>Notable Top-5% Paper</b>.

  <span class="press">
  Press:
    <a href="https://www.vice.com/en/article/4axjnm/scientists-made-discovery-about-how-ai-actually-works">Motherboard</a>,
    <a href="https://news.mit.edu/2023/large-language-models-in-context-learning-0207">MIT News</a>.
  </span>
  <a href="https://github.com/ekinakyurek/google-research/tree/master/incontext" class="code">code</a>

- [Characterizing intrinsic compositionality in transformers with tree projections.](https://arxiv.org/abs/2211.01288)

  Shikhar Murty, Pratyusha Sharma, Jacob Andreas and Christopher Manning. ICLR 2023.

- [Compositional semantic parsing with large language models.](https://arxiv.org/abs/2209.15003)

  Andrew Drozdov, Nathanael Schärli, Ekin Aky&uuml;rek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou. ICLR 2023.

- [Mastering the game of no-press Diplomacy via human-regularized reinforcement learning and planning.](https://arxiv.org/abs/2210.05492)

  Anton Bakhtin\*, David J Wu\*, Adam Lerer\*, Jonathan Gray\*, Athul Paul Jacob\*, Gabriele Farina\*, Alexander H Miller, Noam Brown.

  ICLR 2023 <b>Notable Top-5% Paper</b>.
  
  <span class="press">Press: <a href="https://www.newscientist.com/article/2343027-ais-built-by-meta-beat-human-experts-at-diplomacy/">NewScientist</a></span>
  
- [Top-down synthesis for library learning.](https://arxiv.org/abs/2211.16605)

  Matthew Bowers, Theo X. Olausson, Lionel Wong, Gabriel Grand, Joshua B. Tenenbaum, Kevin Ellis, Armando Solar-Lezama.

  POPL 2023.

  <a class="code" href="https://github.com/mlb2251/stitch">code</a>

### 2022  

- [Pre-trained language models for interactive decision-making.](https://arxiv.org/abs/2202.01771)

  Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Aky&uuml;rek, Anima Anandkumar\*, Jacob Andreas\*, Igor Mordatch\*, Antonio Torralba\*, Yuke Zhu\*.

  NeurIPS 2022 <b>Oral</b>.

  <a class="project" href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/">project page</a>

- [Human-level play in the game of Diplomacy by combining language models with strategic reasoning.](https://www.science.org/doi/10.1126/science.ade9097)
  
  FAIR, Anton Bakhtin\* , Noam Brown\*, Emily Dinan\*, Gabriele Farina, Colin Flaherty\*, Daniel Fried, Andrew Goff, Jonathan Gray\*, Hengyuan Hu\*, Athul Paul Jacob\*, Mojtaba  Komeili, Karthik Konath, Minae Kwon, Adam Lerer\*, Mike Lewis\*, Alexander H. Miller\*, Sasha Mitts, Adithya Renduchintala\*, Stephen Roller, Dirk Rowe, Weiyan Shi\*, Joe Spisak, Alexander Wei, David Wu\*, Hugh Zhang\*, Markus Zijlstra. Science 2022.

  <span class="press">Press:
    <a href="https://www.economist.com/science-and-technology/2022/11/23/another-game-falls-to-an-ai-player">The Economist</a>,
    <a href="https://www.forbes.com/sites/carlieporterfield/2022/11/22/metas-ai-gamer-beat-humans-in-diplomacy-using-strategy-and-negotiation/?sh=3d2ce522788b">Forbes</a>,
    <a href="https://www.washingtonpost.com/technology/2022/12/01/meta-diplomacy-ai-cicero/">The Washington Post</a>,
    <a href="https://www.nytimes.com/2023/01/20/technology/chatbots-turing-test.html">The New York Times</a>,
    <a href="https://www.newyorker.com/culture/2022-in-review/eight-times-science-exceeded-expectations-in-2022">The New Yorker</a>,
    <a href="https://venturebeat.com/ai/is-ai-moving-too-fast-for-ethics-the-ai-beat/">VentureBeat</a>.
  </span>
  
- [Language models as agent models.](https://arxiv.org/abs/2212.01681)
  
  Jacob Andreas.

  EMNLP Findings 2022.

- [Toward tracing factual knowledge in language models back to the training data.](https://arxiv.org/abs/2205.11482)

  Ekin Aky&uuml;rek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, Kelvin Guu.

  EMNLP Findings 2022.

  <a class="code" href="https://github.com/ekinakyurek/influence">code</a>

- [Hierarchical phrase-based sequence-to-sequence learning.](https://arxiv.org/abs/2211.07906)
 
  Bailin Wang, Ivan Titov, Jacob Andreas and Yoon Kim.

  EMNLP 2022.

  <a class="code" href="https://github.com/berlino/btg-seq2seq">code</a>
  
- [Modeling strong and human-Like gameplay with KL-regularized search.](https://arxiv.org/abs/2112.07544)
  
  Athul Paul Jacob\*, David J. Wu\*, Gabriele Farina\*, Adam Lerer, Hengyuan Hu, Anton Bakhtin, Jacob Andreas, Noam Brown.

  ICML 2022 <b>Spotlight</b>.

- [Toward understanding the communication in sperm whales.](https://www.sciencedirect.com/science/article/pii/S2589004222006642)

  Jacob Andreas, Gašper Beguš, Michael M. Bronstein, Roee Diamant, Denley
  Delaney, Shane Gero, Shafi Goldwasser, David F. Gruber, Sarah de Haas, Peter
  Malkin, Nikolay Pavlov, Roger Payne, Giovanni Petri, Daniela Rus, Pratyusha
  Sharma, Dan Tchernov, Pernille Tønnesen, Antonio Torralba, Daniel Vogt, Robert
  J. Wood.

  iScience 2022.

  <span class="press">Press: <a href="https://www.newyorker.com/magazine/2023/09/11/can-we-talk-to-whales">The New Yorker</a></span>

- [Identifying concept libraries from language about object structure.](https://arxiv.org/abs/2205.05666) 

  Lionel Wong\*, William P. McCarthy\*, Gabriel Grand\*, Yoni Friedman, Joshua B. Tenenbaum, Jacob Andreas, Robert D. Hawkins, Judith E. Fan.

  CogSci 2022.

  <a class="code" href="https://github.com/cogtoolslab/lax-cogsci22">code</a>

- [Correcting robot plans with natural language feedback.](https://arxiv.org/abs/2204.05186)

  Pratyusha Sharma, Balakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Torralba, Jacob Andreas, Dieter Fox.

  RSS 2022.

- [Quantifying adaptability in pre-trained language models with 500 tasks.](https://arxiv.org/abs/2112.03204)

  Belinda Z. Li, Jane Yu, Madian Khabsa, Luke Zettlemoyer, Alon Halevy, Jacob Andreas.

  NAACL 2022.

  <a class="code" href="https://github.com/belindal/TaskBench500">code</a>

- [Skill induction and planning with latent language.](https://arxiv.org/abs/2110.01517)

  Pratyusha Sharma, Antonio Torralba, and Jacob Andreas.

  ACL 2022.

  <a class="project"
href="https://sites.google.com/view/skill-induction-latent-lang/">project
page</a>

- [Natural language descriptions of deep visual features.](https://arxiv.org/abs/2201.11114)

  Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio
  Torralba and Jacob Andreas.

  ICLR 2022 <b>Oral</b>.

  <span class="press">Press: <a href="https://news.mit.edu/2022/explainable-machine-learning-0127">MIT News</a></span>
  <a class="project" href="http://milan.csail.mit.edu/">project page</a>

- [Subspace regularizers for few-shot class incremental learning.](https://arxiv.org/abs/2110.07059)

  Afra Feyza Aky&uuml;rek, Ekin Aky&uuml;rek, Derry Wijaya and Jacob Andreas.

  ICLR 2022.

  <a class="code" href="https://github.com/feyzaakyurek/subspace-reg">code</a>

### 2021

- [Teachable reinforcement learning via advice distillation.](https://arxiv.org/abs/2203.11197)

  Olivia Watkins, Trevor Darrell, Pieter Abbeel, Jacob Andreas and Abhishek Gupta.

  <a class="code" href="https://github.com/rll-research/teachable">code</a>

- [How do neural sequence models generalize? Local and global context cues for
  out-of-distribution prediction.](https://arxiv.org/abs/2111.03108)

  Anthony Bau and Jacob Andreas.

  EMNLP 2021.

- [Toward a visual concept vocabulary for generative adversarial networks](https://arxiv.org/abs/2110.04292)

  Sarah Schwettmann, Evan Hernandez, David Bau, Samuel Klein, Jacob Andreas, Antonio Torralba.

  ICCV 2021.

  <a class="project" href="http://visualvocab.csail.mit.edu/">project page</a>

- [The low-dimensional linear geometry of contextualized word representations.](https://arxiv.org/abs/2105.07109)

  Evan Hernandez and Jacob Andreas.

  CoNLL 2021.

  <a class="code" href="https://github.com/evandez/low-dimensional-probing">code</a>

- [Leveraging language to learn program abstractions and search heuristics.](https://arxiv.org/abs/2106.11053)

  Lionel Wong, Kevin Ellis, Joshua B. Tenenbaum and Jacob Andreas.

  ICML 2021.

  <a class="code" href="https://github.com/ellisk42/ec/tree/icml_2021_supplement">code</a>

- [Implicit representations of meaning in neural language models.](https://arxiv.org/abs/2106.00737)

  Belinda Z. Li, Maxwell Nye and Jacob Andreas.

  ACL 2021.

  <span class="press">Press: <a href="https://www.scientificamerican.com/article/how-ai-knows-things-no-one-told-it/">Scientific American</a></span>
  <a class="code" href="https://github.com/belindal/state-probes">code</a>

- [What context features can transformer language models use?](https://arxiv.org/abs/2106.08367)

  Joe O'Connor and Jacob Andreas.

  ACL 2021.

- [Lexicon learning for few-shot sequence modeling.](https://arxiv.org/abs/2106.03993)

  Ekin Aky&uuml;rek and Jacob Andreas.

  ACL 2021.

  <a class="code" href="https://github.com/ekinakyurek/lexical">code</a>

- [Multitasking inhibits semantic drift](https://arxiv.org/abs/2104.07219).

  Athul Paul Jacob, Mike Lewis and Jacob Andreas.

  NAACL 2021.

- [Representing partial programs with blended abstract
  semantics](https://arxiv.org/abs/2012.12964).

  Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B. Tenenbaum,
  Armando Solar-Lezama.

  ICLR 2021.

- [Learning to recombine and resample data for compositional
  generalization](https://arxiv.org/abs/2010.03706).

  Ekin Aky&uuml;rek, Afra Feyza Aky&uuml;rek and Jacob Andreas.

  ICLR 2021.

  <a class="code" href="https://github.com/ekinakyurek/compgen">code</a>

### 2020

- [Compositional explanations of neurons](https://arxiv.org/abs/2006.14032).

  Jesse Mu and Jacob Andreas.

  NeurIPS 2020 <b>Oral</b>.

  <a class="code" href="https://github.com/jayelm/compexp">code</a>

- [A benchmark for systematic generalization in grounded language
   understanding.](https://arxiv.org/abs/2003.05161)

  Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt and Brenden Lake.
  
  NeurIPS 2020.

  <a class="code" href="https://github.com/LauraRuis/groundedSCAN">code</a>

- [Good-Enough Compositional Data Augmentation](
  https://arxiv.org/abs/1904.09545).

  Jacob Andreas.

  ACL 2020.

  <a class="code" href="https://github.com/jacobandreas/geca">code</a>

- [Unnatural language processing: bridging the gap between synthetic and natural
  language data](https://arxiv.org/abs/2004.13645).

  Alana Marzoev, Sam Madden, Frans Kaashoek, Mike Cafarella and Jacob Andreas.

  NeurIPS workshop on Emergent Communication.

### 2019

- [A survey of reinforcement learning informed by natural language](
  https://arxiv.org/abs/1906.03926).

  Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob
  Andreas, Edward Grefenstette, Shimon Whiteson and Tim Rocktäschel.

  IJCAI 2019.

- [Measuring compositionality in representation learning](
  https://arxiv.org/abs/1902.07181).

  Jacob Andreas.

  ICLR 2019.

  <a href="https://github.com/jacobandreas/tre" class="code">code</a>
